from airflow import DAG
from datetime import timedelta

from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator

import urllib.request
import time
import glob, os
import json

# pull course catalog pages
def catalog():

    #define pull(url) helper function
    def pull(url):
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')

        return data
         
    #define store(data,file) helper function
    def store(data,file):
        with open(file, "w+", encoding="utf-8") as f:
            f.write(data)
        print('wrote file: ' + file)

    # Read URLs from the urls.txt file and create a list
    with open("/opt/airflow/dags/urls.txt", "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]


    for url in urls:
        # Find the index after the last '/' to extract the filename
        index = url.rfind('/') + 1
        file = url[index:]

        # Call pull() to get the data from the URL
        data = pull(url)

        #call store function
        store(data,file)

        print('pulled: ' + file)
        print('--- waiting ---')
        time.sleep(15)


# combine all of the unstructured data files into one large file
def combine():

    # Open combo.txt for writing. 'w' mode will create the file if it doesn't exist and overwrite it if it does.
    with open('combo.txt', 'w', encoding='utf-8') as outfile:
        # Iterate through all files in the current directory that end with .html
        for filename in glob.glob("*.html"):
            # Open each HTML file for reading
            with open(filename) as infile:
                # Read the contents of the current file and write it to combo.txt
                outfile.write(infile.read())
                outfile.write("\n")

    print("Combined files into combo.txt")


# build an array of course titles
def titles():
      
    from bs4 import BeautifulSoup

    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print('wrote file: ' + file)

   #Open and read the large html file generated by combine()
    f = open('combo.txt', 'r')
    html = f.read()

    #the following replaces new line and carriage return char
    html = html.replace('\n', ' ').replace('\r', '')
    #the following create an html parser
    soup = BeautifulSoup(html, "html.parser")
    #find course titles which have an <h3> tag
    results = soup.find_all('h3')
   
    titles = []
    # tag inner text
    for item in results:
        titles.append(item.text)

    store_json(titles, 'titles.json')


# remove all punctuation, numbers, and one-character words
def clean():
   
   #complete helper function definition below
   def store_json(data,file):
       with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print('wrote file: ' + file)

   with open('titles.json', 'r') as file:
       titles = json.load(file)

       # remove punctuation/numbers
       for index, title in enumerate(titles):
           punctuation= '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
           translationTable= str.maketrans("","",punctuation)
           clean = title.translate(translationTable)
           titles[index] = clean

       # remove one character words
       for index, title in enumerate(titles):
           clean = ' '.join( [word for word in title.split() if len(word)>1] )
           titles[index] = clean
           
       store_json(titles, 'titles_clean.json')


# counts word frequency
def count_words():

    from collections import Counter

    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('wrote file: ' + file)

    with open('titles_clean.json', 'r') as file:
        titles = json.load(file)

        words = []
        # extract words and flatten
        for title in titles:
            words.extend(title.split())

        # count word frequency
        counts = Counter(words)
    
        store_json(counts, 'words.json')


with DAG(
   "Project",
   start_date=days_ago(1),
   schedule_interval="@daily",catchup=False,
) as dag:

   # ts are tasks
   t0 = BashOperator(
       task_id='task_zero',
       bash_command='pip install beautifulsoup4',
       retries=2
   )
   t1 = PythonOperator(
       task_id='task_one',
       depends_on_past=False,
       python_callable=catalog
   )
   t2 = PythonOperator(
       task_id='task_two',
       depends_on_past=False,
       python_callable=combine
   )
   t3 = PythonOperator(
       task_id='task_three',
       depends_on_past=False,
       python_callable=titles
   )
   t4 = PythonOperator(
       task_id='task_four',
       depends_on_past=False,
       python_callable=clean
   )
   t5 = PythonOperator(
       task_id='task_five',
       depends_on_past=False,
       python_callable=count_words
   )

   t0>>t1>>t2>>t3>>t4>>t5